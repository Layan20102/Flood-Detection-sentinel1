{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Live Testing"
      ],
      "metadata": {
        "id": "N-s1_MkB5bdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook allows you to **test the flood-detection models on real Sentinel-1 SAR data**.\n",
        "\n",
        "To use it:\n",
        "\n",
        "1. Prepare two GeoTIFF files for the **same area and same date**:  \n",
        "   - one **VV** band file (e.g., `*_VV.tif`)  \n",
        "   - one **VH** band file (e.g., `*_VH.tif`)\n",
        "2. Make sure you **update the model paths** in the code if your `.keras`, `.pkl`, or `.joblib` files are stored in a different Drive folder.\n",
        "3. Choose the model you want to test from the **Model Selection** section (CNN, Random Forest, or SVM).\n",
        "4. Upload both VV and VH files in the **Upload Sentinel-1 GeoTIFFs** area.\n",
        "5. The app will automatically:\n",
        "   - convert VV/VH into 6 SAR-based features,  \n",
        "   - split the scene into 256Ã—256 tiles,  \n",
        "   - apply the selected model to each tile,  \n",
        "   - compute the flood probability map,  \n",
        "   - and provide a final **scene-level flood decision**.\n",
        "6. Review the outputs:\n",
        "   - model prediction (flooded / non-flooded)  \n",
        "   - tile statistics  \n",
        "   - **heatmap** of probabilities (blue = low, red = high)\n",
        "\n"
      ],
      "metadata": {
        "id": "CX77BNpxWN1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Live Testing - installs\n",
        "\n",
        "!pip install streamlit -q\n",
        "!pip -q install \"rasterio>=1.4.2\" \"scipy\" \"pillow\"\n",
        "!pip -q install -U \"numpy>=2.0\" \"ml-dtypes>=0.5.0\"\n",
        "!pip -q install -U rasterio tifffile pillow scipy matplotlib\n",
        "!pip -q install -U \"tensorflow==2.19.0\" \"tf-keras==2.19.0\" \"tensorflow-text==2.19.0\"\n",
        "\n",
        "\n",
        "import base64, wave\n",
        "import streamlit.components.v1 as components\n",
        "!pkill -f streamlit\n",
        "import time\n",
        "time.sleep(3)\n",
        "print(\"âœ… ØªÙ… Ø¥ÙŠÙ‚Ø§Ù ØªØ·Ø¨ÙŠÙ‚Ø§Øª Streamlit\")\n",
        "\n",
        "!ps aux | grep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtCj4_XJVNif",
        "outputId": "07e9a9cf-3463-4055-af72-8d1a411cbabb",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… ØªÙ… Ø¥ÙŠÙ‚Ø§Ù ØªØ·Ø¨ÙŠÙ‚Ø§Øª Streamlit\n",
            "root        7733  0.0  0.0   7376  3512 ?        S    17:20   0:00 /bin/bash -c ps aux | grep streamlit\n",
            "root        7735  0.0  0.0   6484  2388 ?        S    17:20   0:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "paths = {\n",
        "    \"CNN (Deep Learning)\": \"/content/drive/MyDrive/sen12flood_npz_full_6feats/best_model.keras\",\n",
        "    \"Random Forest\": \"/content/drive/MyDrive/best_rf_model.pkl\",\n",
        "    \"SVM\": \"/content/drive/MyDrive/sen12flood_npz_full_6feats/svm_results/svm_rbf_approx.joblib\",\n",
        "}\n",
        "\n",
        "for name, p in paths.items():\n",
        "    print(name, \"â†’\", \"OK âœ…\" if os.path.exists(p) else \"NOT FOUND âŒ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRf7yxeSXO7n",
        "outputId": "e04e3614-db9c-499f-a69a-18096a339654",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "CNN (Deep Learning) â†’ OK âœ…\n",
            "Random Forest â†’ OK âœ…\n",
            "SVM â†’ OK âœ…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile simple_flood_app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import io, base64, wave\n",
        "from typing import Tuple, List\n",
        "from scipy.ndimage import uniform_filter\n",
        "from rasterio.io import MemoryFile\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import joblib\n",
        "import streamlit.components.v1 as components\n",
        "\n",
        "# =========================\n",
        "# Page configuration\n",
        "# =========================\n",
        "st.set_page_config(page_title=\"Flood Detection (S1)\", page_icon=\"ğŸŒŠ\", layout=\"centered\")\n",
        "st.title(\"ğŸŒŠ Flood Detection System\")\n",
        "st.caption(\n",
        "    \"Upload two GeoTIFF files (VV and VH) captured after the event. \"\n",
        "    \"The app converts them into 6 channels and uses the selected model (256Ã—256Ã—6 â†’ 1) \"\n",
        "    \"to decide whether the scene is flooded or not.\"\n",
        ")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# =========================\n",
        "# Model file paths (CHANGE IF NEEDED)\n",
        "# =========================\n",
        "MODEL_PATHS = {\n",
        "    \"CNN (Deep Learning)\": \"/content/drive/MyDrive/sen12flood_npz_full_6feats/best_model.keras\",\n",
        "    \"Random Forest\": \"/content/drive/MyDrive/best_rf_model.pkl\",\n",
        "    \"SVM\": \"/content/drive/MyDrive/sen12flood_npz_full_6feats/svm_results/svm_rbf_approx.joblib\",\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# Model selection\n",
        "# =========================\n",
        "st.subheader(\"âš™ï¸ Model Selection\")\n",
        "model_choice = st.radio(\n",
        "    \"Choose which model you want to use for live testing:\",\n",
        "    list(MODEL_PATHS.keys()),\n",
        "    index=0,\n",
        ")\n",
        "\n",
        "st.info(f\"You selected: **{model_choice}**\")\n",
        "\n",
        "# =========================\n",
        "# Load models (cached)\n",
        "# =========================\n",
        "@st.cache_resource(show_spinner=True)\n",
        "def load_cnn_model(path: str):\n",
        "    return keras.models.load_model(path)\n",
        "\n",
        "@st.cache_resource(show_spinner=True)\n",
        "def load_sklearn_model(path: str):\n",
        "    \"\"\"\n",
        "    Loads classical ML models.\n",
        "\n",
        "    - For SVM (svm_rbf_approx.joblib): returns the full tuple (scaler, rbf, clf)\n",
        "    - For other models (e.g. Random Forest): returns the estimator itself\n",
        "    \"\"\"\n",
        "    import pickle, os\n",
        "\n",
        "    basename = os.path.basename(path).lower()\n",
        "\n",
        "    try:\n",
        "        obj = joblib.load(path)\n",
        "    except Exception:\n",
        "        with open(path, \"rb\") as f:\n",
        "            obj = pickle.load(f)\n",
        "\n",
        "    #  SVM case: keep the full tuple\n",
        "    if \"svm\" in basename:\n",
        "        return obj   # (scaler, rbf, calibrated_svm)\n",
        "\n",
        "    #  Other models (e.g. Random Forest)\n",
        "    if isinstance(obj, (list, tuple)) and len(obj) > 0:\n",
        "        return obj[0]\n",
        "\n",
        "    if isinstance(obj, dict):\n",
        "        for key in [\"model\", \"clf\", \"estimator\", \"rf\", \"classifier\"]:\n",
        "            if key in obj:\n",
        "                return obj[key]\n",
        "\n",
        "    return obj\n",
        "\n",
        "def get_selected_model(choice: str):\n",
        "    \"\"\"Return the loaded model object based on the user choice.\"\"\"\n",
        "    path = MODEL_PATHS[choice]\n",
        "    if \"CNN\" in choice:\n",
        "        return load_cnn_model(path)\n",
        "    else:\n",
        "        return load_sklearn_model(path)\n",
        "\n",
        "try:\n",
        "    model = get_selected_model(model_choice)\n",
        "    model_loaded_ok = True\n",
        "except Exception as e:\n",
        "    st.error(f\"Error loading **{model_choice}** model from `{MODEL_PATHS[model_choice]}`: {e}\")\n",
        "    model = None\n",
        "    model_loaded_ok = False\n",
        "\n",
        "# =========================\n",
        "# Hidden audio helpers\n",
        "# =========================\n",
        "def _html_audio_hidden_from_bytes(b: bytes, loop: bool = False, autoplay: bool = True, height: int = 0):\n",
        "    b64 = base64.b64encode(b).decode(\"ascii\")\n",
        "    loop_attr = \" loop\" if loop else \"\"\n",
        "    autoplay_attr = \" autoplay\" if autoplay else \"\"\n",
        "    components.html(\n",
        "        f\"\"\"\n",
        "        <audio{autoplay_attr}{loop_attr} style=\"display:none\">\n",
        "          <source src=\"data:audio/wav;base64,{b64}\" type=\"audio/wav\">\n",
        "        </audio>\n",
        "        \"\"\",\n",
        "        height=height,\n",
        "    )\n",
        "\n",
        "def generate_alarm_wav(seconds: float = 2.5, rate: int = 22050) -> io.BytesIO:\n",
        "    buf = io.BytesIO()\n",
        "    with wave.open(buf, 'wb') as wf:\n",
        "        wf.setnchannels(1)\n",
        "        wf.setsampwidth(2)\n",
        "        wf.setframerate(rate)\n",
        "\n",
        "        beep_freq = 900.0\n",
        "        beep_dur = 0.15\n",
        "        gap_dur  = 0.25\n",
        "        total_dur = seconds\n",
        "\n",
        "        pattern = []\n",
        "        elapsed = 0.0\n",
        "        while elapsed < total_dur:\n",
        "            t_beep = np.linspace(0, beep_dur, int(rate * beep_dur), False)\n",
        "            tone = 0.6 * np.sin(2 * np.pi * beep_freq * t_beep)\n",
        "            fade_len = max(1, int(0.02 * rate))\n",
        "            if tone.size >= 2 * fade_len:\n",
        "                tone[:fade_len] *= np.linspace(0.0, 1.0, fade_len)\n",
        "                tone[-fade_len:] *= np.linspace(1.0, 0.0, fade_len)\n",
        "            pattern.append(tone); elapsed += beep_dur\n",
        "\n",
        "            if elapsed < total_dur:\n",
        "                gap = np.zeros(int(rate * gap_dur))\n",
        "                pattern.append(gap); elapsed += gap_dur\n",
        "\n",
        "        snd = np.concatenate(pattern, axis=0)\n",
        "        snd = np.clip(snd, -0.99, 0.99)\n",
        "        wf.writeframes((snd * 32767).astype(np.int16).tobytes())\n",
        "\n",
        "    buf.seek(0)\n",
        "    return buf\n",
        "\n",
        "def generate_all_clear_wav(rate: int = 22050) -> io.BytesIO:\n",
        "    def tone(f_hz, sec, amp=0.5):\n",
        "        t = np.linspace(0, sec, int(rate * sec), False)\n",
        "        snd = amp * np.sin(2 * np.pi * f_hz * t)\n",
        "        fade_len = max(1, int(0.03 * rate))\n",
        "        env = np.ones_like(snd)\n",
        "        env[:fade_len] *= np.linspace(0.0, 1.0, fade_len)\n",
        "        env[-fade_len:] *= np.linspace(1.0, 0.0, fade_len)\n",
        "        return snd * env\n",
        "\n",
        "    seg1 = tone(880.0, 0.55, 0.5)\n",
        "    pause = np.zeros(int(rate * 0.08))\n",
        "    seg2 = tone(660.0, 0.55, 0.5)\n",
        "    snd = np.concatenate([seg1, pause, seg2], axis=0)\n",
        "    snd = np.clip(snd, -0.99, 0.99)\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    with wave.open(buf, 'wb') as wf:\n",
        "        wf.setnchannels(1)\n",
        "        wf.setsampwidth(2)\n",
        "        wf.setframerate(rate)\n",
        "        wf.writeframes((snd * 32767).astype(np.int16).tobytes())\n",
        "    buf.seek(0)\n",
        "    return buf\n",
        "\n",
        "def play_alarm_loop_hidden():\n",
        "    _html_audio_hidden_from_bytes(generate_alarm_wav().getvalue(), loop=True, autoplay=True)\n",
        "\n",
        "def play_all_clear_once_hidden():\n",
        "    _html_audio_hidden_from_bytes(generate_all_clear_wav().getvalue(), loop=False, autoplay=True)\n",
        "\n",
        "# =========================\n",
        "# SAR â†’ dB â†’ 6 features\n",
        "# =========================\n",
        "def to_db(x: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
        "    x = np.clip(x, eps, None)\n",
        "    out = 10.0 * np.log10(x)\n",
        "    out[~np.isfinite(out)] = 0.0\n",
        "    return out.astype(np.float32)\n",
        "\n",
        "def lee_filter(img: np.ndarray, size: int = 5) -> np.ndarray:\n",
        "    img = img.astype(np.float32)\n",
        "    mean = uniform_filter(img, size=size)\n",
        "    mean_sq = uniform_filter(img**2, size=size)\n",
        "    var = mean_sq - mean**2\n",
        "    overall_var = np.nanmean(var)\n",
        "    w = var / (var + overall_var + 1e-6)\n",
        "    return (mean + w * (img - mean)).astype(np.float32)\n",
        "\n",
        "def mk_6_features(vv_db: np.ndarray, vh_db: np.ndarray) -> np.ndarray:\n",
        "    vv_lin = 10.0 ** (vv_db / 10.0)\n",
        "    vh_lin = 10.0 ** (vh_db / 10.0)\n",
        "    feats = np.stack([\n",
        "        vv_db,                    # 1) VV dB\n",
        "        vh_db,                    # 2) VH dB\n",
        "        vv_db + vh_db,            # 3) VV + VH (dB)\n",
        "        vv_db - vh_db,            # 4) VV - VH (dB)\n",
        "        vv_lin / (vh_lin + 1e-6), # 5) VV/VH (linear)\n",
        "        vv_lin * vh_lin           # 6) VV*VH (linear)\n",
        "    ], axis=0).astype(np.float32)\n",
        "    feats = np.nan_to_num(feats, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return feats\n",
        "\n",
        "def normalize_6ch(x6: np.ndarray, mean=None, std=None) -> np.ndarray:\n",
        "    x = x6.copy().astype(np.float32)\n",
        "    if mean is not None and std is not None:\n",
        "        mean = np.array(mean, dtype=np.float32).reshape(6, 1, 1)\n",
        "        std  = np.array(std, dtype=np.float32).reshape(6, 1, 1)\n",
        "        std  = np.where(std == 0, 1.0, std)\n",
        "        return (x - mean) / std\n",
        "    ch_mean = x.reshape(6, -1).mean(axis=1).reshape(6,1,1)\n",
        "    ch_std  = x.reshape(6, -1).std(axis=1).reshape(6,1,1)\n",
        "    ch_std  = np.where(ch_std == 0, 1.0, ch_std)\n",
        "    return (x - ch_mean) / ch_std\n",
        "\n",
        "def read_band_from_tif(uploaded) -> Tuple[np.ndarray, dict]:\n",
        "    raw = uploaded.read()\n",
        "    with MemoryFile(raw) as memfile:\n",
        "        with memfile.open() as src:\n",
        "            arr = src.read(1).astype(np.float32)\n",
        "            meta = {\n",
        "                \"crs\": src.crs,\n",
        "                \"transform\": src.transform,\n",
        "                \"height\": src.height,\n",
        "                \"width\": src.width,\n",
        "            }\n",
        "    return arr, meta\n",
        "\n",
        "def detect_polarization_from_name(name: str):\n",
        "    n = name.upper()\n",
        "    if \"VV\" in n and \"VH\" in n:\n",
        "        return \"BOTH\"\n",
        "    if \"VV\" in n:\n",
        "        return \"VV\"\n",
        "    if \"VH\" in n:\n",
        "        return \"VH\"\n",
        "    return None\n",
        "\n",
        "def tile_coords(H:int, W:int, tile_size:int=256, stride:int=256) -> List[Tuple[int,int]]:\n",
        "    coords = []\n",
        "    for y in range(0, H - tile_size + 1, stride):\n",
        "        for x in range(0, W - tile_size + 1, stride):\n",
        "            coords.append((y, x))\n",
        "    return coords\n",
        "\n",
        "# =========================\n",
        "# Unified prediction helper\n",
        "# =========================\n",
        "def predict_tile_probs(model_obj, model_type: str, X_tiles: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Returns probabilities for each tile for the selected model type.\n",
        "    - CNN: per-tile standardization + model.predict (sigmoid output)\n",
        "    - Random Forest: flatten â†’ predict_proba\n",
        "    - SVM (approx RBF): scaler â†’ rbf â†’ calibrated_svm.predict_proba\n",
        "    \"\"\"\n",
        "    X_tiles = X_tiles.astype(np.float32)\n",
        "\n",
        "    # ========= CNN =========\n",
        "    if \"CNN\" in model_type:\n",
        "        mu = X_tiles.mean(axis=(1, 2), keepdims=True)\n",
        "        sd = X_tiles.std(axis=(1, 2), keepdims=True)\n",
        "        X_std = (X_tiles - mu) / (sd + 1e-8)\n",
        "\n",
        "        probs = model_obj.predict(X_std, verbose=0).reshape(-1)\n",
        "        return probs.astype(np.float32)\n",
        "\n",
        "    N = X_tiles.shape[0]\n",
        "    X_flat = X_tiles.reshape(N, -1)\n",
        "\n",
        "    # ========= SVM (approx RBF) =========\n",
        "    if \"SVM\" in model_type:\n",
        "        # ØªÙˆÙ‚Ø¹ÙŠ: model_obj = (scaler, rbf, clf)\n",
        "        if isinstance(model_obj, (list, tuple)) and len(model_obj) == 3:\n",
        "            scaler, rbf, clf = model_obj\n",
        "        elif isinstance(model_obj, dict):\n",
        "            scaler = model_obj.get(\"scaler\")\n",
        "            rbf    = model_obj.get(\"rbf\")\n",
        "            clf    = model_obj.get(\"model\")\n",
        "        else:\n",
        "            raise ValueError(\"Unexpected SVM model format. Expected (scaler, rbf, clf).\")\n",
        "\n",
        "        X_scaled = scaler.transform(X_flat)\n",
        "        X_rbf = rbf.transform(X_scaled)\n",
        "        probs = clf.predict_proba(X_rbf)[:, 1]\n",
        "        return probs.astype(np.float32)\n",
        "\n",
        "    # ========= Random Forest =========\n",
        "    if hasattr(model_obj, \"predict_proba\"):\n",
        "        probs = model_obj.predict_proba(X_flat)[:, 1]\n",
        "    else:\n",
        "        if hasattr(model_obj, \"decision_function\"):\n",
        "            scores = model_obj.decision_function(X_flat)\n",
        "            probs = 1.0 / (1.0 + np.exp(-scores))\n",
        "        else:\n",
        "            preds = model_obj.predict(X_flat)\n",
        "            probs = np.asarray(preds, dtype=np.float32)\n",
        "\n",
        "    return probs.astype(np.float32)\n",
        "\n",
        "# =========================\n",
        "# Upload & processing interface\n",
        "# =========================\n",
        "st.subheader(\"ğŸ“¤ Upload Sentinel-1 GeoTIFFs\")\n",
        "files = st.file_uploader(\n",
        "    \"Upload VV and VH (GeoTIFF) files â€” post-event\",\n",
        "    type=[\"tif\", \"tiff\"],\n",
        "    accept_multiple_files=True\n",
        ")\n",
        "\n",
        "if files and model_loaded_ok and model is not None:\n",
        "    vv_file = None\n",
        "    vh_file = None\n",
        "    for f in files:\n",
        "        pol = detect_polarization_from_name(f.name)\n",
        "        if pol == \"VV\" and vv_file is None:\n",
        "            vv_file = f\n",
        "        elif pol == \"VH\" and vh_file is None:\n",
        "            vh_file = f\n",
        "\n",
        "    if vv_file is None or vh_file is None:\n",
        "        st.error(\"Please ensure one file name contains 'VV' and the other 'VH' (e.g., *_VV_.tif and *_VH_.tif).\")\n",
        "    else:\n",
        "        with st.spinner(\"Reading VV/VH and generating features...\"):\n",
        "            try:\n",
        "                vv_lin, meta_vv = read_band_from_tif(vv_file)\n",
        "                vh_lin, meta_vh = read_band_from_tif(vh_file)\n",
        "\n",
        "                if vv_lin.shape != vh_lin.shape:\n",
        "                    st.error(f\"VV dimensions ({vv_lin.shape}) â‰  VH dimensions ({vh_lin.shape}). Both must be aligned.\")\n",
        "                    st.stop()\n",
        "\n",
        "                # Display uploaded bands (linear)\n",
        "                st.markdown(\"### ğŸ“¸ Uploaded Bands\")\n",
        "                col1, col2 = st.columns(2)\n",
        "                with col1:\n",
        "                    st.write(\"**VV Band (linear)**\")\n",
        "                    st.image(vv_lin, caption=\"VV\", use_column_width=True, clamp=True)\n",
        "                with col2:\n",
        "                    st.write(\"**VH Band (linear)**\")\n",
        "                    st.image(vh_lin, caption=\"VH\", use_column_width=True, clamp=True)\n",
        "\n",
        "                H, W = vv_lin.shape\n",
        "                vv_db = to_db(vv_lin)\n",
        "                vh_db = to_db(vh_lin)\n",
        "                vv_db = lee_filter(vv_db, size=5)\n",
        "                vh_db = lee_filter(vh_db, size=5)\n",
        "\n",
        "                feats6 = mk_6_features(vv_db, vh_db)\n",
        "                feats6 = normalize_6ch(feats6)\n",
        "\n",
        "                coords = tile_coords(H, W, tile_size=256, stride=256)\n",
        "                X_tiles = []\n",
        "                for (y, x0) in coords:\n",
        "                    patch = feats6[:, y:y+256, x0:x0+256]\n",
        "                    if patch.shape[1] != 256 or patch.shape[2] != 256:\n",
        "                        continue\n",
        "                    patch = np.transpose(patch, (1, 2, 0)).astype(np.float32)\n",
        "                    X_tiles.append(patch)\n",
        "\n",
        "                if not X_tiles:\n",
        "                    st.error(\"No 256Ã—256 tiles found for the current dimensions.\")\n",
        "                    st.stop()\n",
        "\n",
        "                X = np.stack(X_tiles, axis=0)\n",
        "\n",
        "                # === Model prediction (depends on selected model) ===\n",
        "                probs = predict_tile_probs(model, model_choice, X)\n",
        "                probs = probs.reshape(-1)\n",
        "\n",
        "                mean_p = float(np.mean(probs))\n",
        "                max_p  = float(np.max(probs))\n",
        "                pct_above = float((probs >= 0.5).mean() * 100.0)\n",
        "\n",
        "                is_flood = (pct_above >= 20.0) or (mean_p >= 0.5) or (max_p >= 0.8)\n",
        "                predicted_label = \"Flooded\" if is_flood else \"Non-flooded\"\n",
        "\n",
        "                st.subheader(\"ğŸ“Š Scene-level Result\")\n",
        "                if is_flood:\n",
        "                    st.error(f\"## ğŸš¨ FLOOD DETECTED ({model_choice})\")\n",
        "                    play_alarm_loop_hidden()\n",
        "                else:\n",
        "                    st.success(f\"## âœ… NON-FLOOD AREA ({model_choice})\")\n",
        "                    play_all_clear_once_hidden()\n",
        "\n",
        "                c1, c2, c3, c4 = st.columns(4)\n",
        "                c1.metric(\"Tiles\", f\"{len(probs)}\")\n",
        "                c2.metric(\"Mean prob\", f\"{mean_p:.3f}\")\n",
        "                c3.metric(\"Max prob\", f\"{max_p:.3f}\")\n",
        "                c4.metric(\"% tiles â‰¥ 0.50\", f\"{pct_above:.1f}%\")\n",
        "\n",
        "                # =====================================================================\n",
        "                # Tile probability map\n",
        "                # =====================================================================\n",
        "                st.markdown(\"---\")\n",
        "                st.subheader(\"ğŸ§© Tile scores (coarse grid)\")\n",
        "\n",
        "                cols = (W - 256) // 256 + 1\n",
        "                rows = (H - 256) // 256 + 1\n",
        "                grid = np.full((rows, cols), np.nan, dtype=np.float32)\n",
        "\n",
        "                idx = 0\n",
        "                for r in range(rows):\n",
        "                    for c in range(cols):\n",
        "                        if idx < len(probs):\n",
        "                            grid[r, c] = probs[idx]\n",
        "                            idx += 1\n",
        "\n",
        "                fig = plt.figure(figsize=(6, 4))\n",
        "                plt.imshow(\n",
        "                    grid,\n",
        "                    vmin=0.0,\n",
        "                    vmax=1.0,\n",
        "                    cmap=\"coolwarm\"\n",
        "                )\n",
        "                cbar = plt.colorbar(label=\"Flood probability\")\n",
        "                cbar.ax.set_ylabel(\"Flood probability\", rotation=270, labelpad=15)\n",
        "                plt.title(\"Tile probability map (coarse)\")\n",
        "                plt.xlabel(\"tile col\")\n",
        "                plt.ylabel(\"tile row\")\n",
        "                st.pyplot(fig)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"An error occurred during reading/processing: {e}\")\n",
        "\n",
        "else:\n",
        "    if not model_loaded_ok or model is None:\n",
        "        st.warning(f\"Model **{model_choice}** could not be loaded. Check the path or Google Drive mounting.\")\n",
        "    st.info(\"Choose a model, then upload both GeoTIFF files (VV and VH) to run the analysis.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HjyQ2J4VXNe",
        "outputId": "b64c8504-2f83-4e48-d577-7f6380bdd02d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting simple_flood_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f streamlit\n",
        "import time\n",
        "time.sleep(3)\n",
        "\n",
        "\n",
        "APP = \"/content/simple_flood_app.py\"\n",
        "\n",
        "import os, re, time, subprocess\n",
        "from google.colab import output\n",
        "\n",
        "subprocess.run([\"pkill\",\"-f\",\"streamlit run\"], check=False)\n",
        "subprocess.run([\"pip\",\"-q\",\"install\",\"-U\",\n",
        "                \"streamlit==1.39.0\",\"pillow\",\"opencv-python\",\"protobuf==3.20.*\"], check=True)\n",
        "\n",
        "log = open(\"/content/streamlit.log\",\"w\")\n",
        "p = subprocess.Popen([\n",
        "    \"streamlit\",\"run\",APP,\n",
        "    \"--server.port\",\"8501\",\n",
        "    \"--server.address\",\"0.0.0.0\",\n",
        "    \"--server.headless\",\"true\",\n",
        "    \"--server.enableCORS\",\"false\",\n",
        "    \"--server.enableXsrfProtection\",\"false\",\n",
        "    \"--browser.gatherUsageStats\",\"false\",\n",
        "    \"--server.fileWatcherType\",\"none\",\n",
        "], stdout=log, stderr=log, text=True)\n",
        "\n",
        "print(\" Waiting 10s, then opening URL...\")\n",
        "time.sleep(10)\n",
        "url = output.eval_js(\"google.colab.kernel.proxyPort(8501)\")\n",
        "print(\" OPEN THIS URL:\\n\", url)\n",
        "print(\"\\n Logs: !tail -n 200 /content/streamlit.log\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Zo13_wXTVZlS",
        "outputId": "2c743fed-47fa-47b8-ec7f-cf86969f2c45"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Waiting 10s, then opening URL...\n",
            " OPEN THIS URL:\n",
            " https://8501-m-s-2a1ffaeft70dh-d.us-east1-0.prod.colab.dev\n",
            "\n",
            " Logs: !tail -n 200 /content/streamlit.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#  Test Samples Evaluation\n",
        "\n",
        "This section evaluates the three trained models â€” **CNN**, **Random Forest**, and **SVM (RBF-Approx.)** â€” using a small set of **5 curated test samples** taken from the SEN12FLOOD dataset.\n",
        "\n",
        "Each sample is a **256Ã—256 tile** containing:\n",
        "\n",
        "* `x_after`: 6 engineered SAR features\n",
        "* `y`: pixel-level flood mask\n",
        "\n",
        "For each file, the script:\n",
        "\n",
        "1. Loads the processed tile.\n",
        "2. Computes the **truth label** using the flood-pixel ratio (`pos_ratio â‰¥ 0.01 â†’ flooded`).\n",
        "3. Applies consistent preprocessing:\n",
        "\n",
        "   * reshaping (6,256,256 â†’ 256,256,6)\n",
        "   * per-tile standardization for CNN\n",
        "4. Runs predictions using:\n",
        "\n",
        "   * the trained **CNN model**\n",
        "   * the optimized **Random Forest model**\n",
        "   * the **RBF-SVM approximate model** (scaler â†’ RBF â†’ calibrated SVM)\n",
        "5. Prints a full comparison for each sample:\n",
        "\n",
        "   * **Ground truth**\n",
        "   * **Predicted label**\n",
        "   * **Prediction probability** for each model\n",
        "\n",
        "This helps verify:\n",
        "\n",
        "* whether the models are loaded correctly\n",
        "* that preprocessing is consistent\n",
        "* and how each model behaves on real SAR tiles\n",
        "\n",
        "> **Reminder:**\n",
        "> Do not forget to update your model paths (`cnn_path`, `rf_path`, `svm_path`) if your files are stored in a different Drive location.\n",
        "\n"
      ],
      "metadata": {
        "id": "8my8q2Bk51KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import joblib, pickle\n",
        "import os\n",
        "\n",
        "mini_test_paths = [\n",
        "    \"/content/drive/MyDrive/mini_test_5/sen12floods_s1_labels_0020_2019_01_16_0128_0256_A.npz\",\n",
        "    \"/content/drive/MyDrive/mini_test_5/sen12floods_s1_labels_46_2019_02_23_0256_0128_A.npz\",\n",
        "    \"/content/drive/MyDrive/mini_test_5/sen12floods_s1_labels_54_2019_03_14_0128_0000_A.npz\",\n",
        "    \"/content/drive/MyDrive/mini_test_5/sen12floods_s1_labels_58_2019_04_14_0000_0256_A.npz\",\n",
        "    \"/content/drive/MyDrive/mini_test_5/sen12floods_s1_labels_0140_2019_03_13_0000_0000_A.npz\",\n",
        "]\n",
        "\n",
        "print(\"Found files:\")\n",
        "for p in mini_test_paths:\n",
        "    print(\" -\", p, \"OK\" if os.path.exists(p) else \"NOT FOUND\")\n",
        "\n",
        "\n",
        "cnn_path = \"/content/drive/MyDrive/sen12flood_npz_full_6feats/best_model.keras\"\n",
        "rf_path  = \"/content/drive/MyDrive/best_rf_model.pkl\"\n",
        "svm_path = \"/content/drive/MyDrive/sen12flood_npz_full_6feats/svm_results/svm_rbf_approx.joblib\"\n",
        "\n",
        "# CNN\n",
        "cnn_model = keras.models.load_model(cnn_path)\n",
        "\n",
        "# Random Forest\n",
        "try:\n",
        "    rf_model = joblib.load(rf_path)\n",
        "except:\n",
        "    with open(rf_path, \"rb\") as f:\n",
        "        rf_model = pickle.load(f)\n",
        "\n",
        "# SVM\n",
        "try:\n",
        "    svm_obj = joblib.load(svm_path)\n",
        "except:\n",
        "    with open(svm_path, \"rb\") as f:\n",
        "        svm_obj = pickle.load(f)\n",
        "\n",
        "if isinstance(svm_obj, (list, tuple)) and len(svm_obj) == 3:\n",
        "    svm_scaler, svm_rbf, svm_clf = svm_obj\n",
        "else:\n",
        "    svm_scaler = svm_obj[\"scaler\"]\n",
        "    svm_rbf    = svm_obj[\"rbf\"]\n",
        "    svm_clf    = svm_obj[\"model\"]\n",
        "\n",
        "def predict_cnn(model, X):\n",
        "    prob = model.predict(X, verbose=0)[0][0]\n",
        "    pred = int(prob >= 0.5)\n",
        "    return pred, float(prob)\n",
        "\n",
        "def predict_rf(model, X, threshold=0.48):\n",
        "    N = X.shape[0] if X.ndim == 4 else 1\n",
        "    X_flat = X.reshape(1, -1).astype(np.float32)\n",
        "    prob = model.predict_proba(X_flat)[0, 1]\n",
        "    pred = int(prob >= threshold)\n",
        "    return pred, float(prob)\n",
        "\n",
        "def predict_svm(scaler, rbf, clf, X):\n",
        "    Xf = X.reshape(1, -1)\n",
        "    Xs = scaler.transform(Xf)\n",
        "    Xr = rbf.transform(Xs)\n",
        "    prob = clf.predict_proba(Xr)[0, 1]\n",
        "    pred = int(prob >= 0.5)\n",
        "    return pred, float(prob)\n",
        "\n",
        "pos_ratio_threshold = 0.01\n",
        "\n",
        "for i, path in enumerate(mini_test_paths):\n",
        "    data = np.load(path)\n",
        "\n",
        "    x_after = data[\"x_after\"].astype(np.float32)\n",
        "    y_mask  = data[\"y\"]\n",
        "\n",
        "    pos_ratio = (y_mask > 0).mean()\n",
        "    y = 1 if pos_ratio >= pos_ratio_threshold else 0\n",
        "\n",
        "    if x_after.ndim == 3 and x_after.shape[0] == 6:\n",
        "        x_hw6 = np.moveaxis(x_after, 0, -1)\n",
        "    elif x_after.ndim == 3:\n",
        "        x_hw6 = x_after\n",
        "    elif x_after.ndim == 4:\n",
        "        x_tmp = x_after[0]\n",
        "        if x_tmp.shape[0] == 6:\n",
        "            x_hw6 = np.moveaxis(x_tmp, 0, -1)\n",
        "        else:\n",
        "            x_hw6 = x_tmp\n",
        "    else:\n",
        "        raise ValueError(f\"Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ù„Ù€ x_after ÙÙŠ {path}: {x_after.shape}\")\n",
        "\n",
        "\n",
        "    mu = x_hw6.mean(axis=(0, 1), keepdims=True)\n",
        "    sd = x_hw6.std(axis=(0, 1), keepdims=True)\n",
        "    x_std = (x_hw6 - mu) / (sd + 1e-8)\n",
        "\n",
        "    X_cnn = x_std.reshape(1, 256, 256, 6)\n",
        "\n",
        "    cnn_prob = float(cnn_model.predict(X_cnn, verbose=0)[0, 0])\n",
        "    cnn_pred = int(cnn_prob >= 0.5)\n",
        "\n",
        "\n",
        "    rf_pred,  rf_prob  = predict_rf(rf_model, x_hw6)\n",
        "    svm_pred, svm_prob = predict_svm(svm_scaler, svm_rbf, svm_clf, x_hw6)\n",
        "\n",
        "    print(f\"Example {i+1}: {os.path.basename(path)}\")\n",
        "    print(f\" - Truth label : {y}  (pos_ratio = {pos_ratio:.3f})\")\n",
        "    print(f\" - CNN         : pred={cnn_pred}  prob={cnn_prob:.4f}\")\n",
        "    print(f\" - RandomForest: pred={rf_pred}   prob={rf_prob:.4f}\")\n",
        "    print(f\" - SVM         : pred={svm_pred}  prob={svm_prob:.4f}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB5Ooxwkrdpz",
        "outputId": "469a25cf-33eb-4ce6-bfb6-4146e0ab6401"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found files:\n",
            " - /content/drive/MyDrive/mini_test_5/sen12floods_s1_labels_0020_2019_01_16_0128_0256_A.npz OK\n",
            " - /content/drive/MyDrive/mini_test_5/sen12floods_s1_labels_46_2019_02_23_0256_0128_A.npz OK\n",
            " - /content/drive/MyDrive/mini_test_5/sen12floods_s1_labels_54_2019_03_14_0128_0000_A.npz OK\n",
            " - /content/drive/MyDrive/mini_test_5/sen12floods_s1_labels_58_2019_04_14_0000_0256_A.npz OK\n",
            " - /content/drive/MyDrive/mini_test_5/sen12floods_s1_labels_0140_2019_03_13_0000_0000_A.npz OK\n",
            "Example 1: sen12floods_s1_labels_0020_2019_01_16_0128_0256_A.npz\n",
            " - Truth label : 0  (pos_ratio = 0.000)\n",
            " - CNN         : pred=0  prob=0.0034\n",
            " - RandomForest: pred=0   prob=0.3848\n",
            " - SVM         : pred=1  prob=0.5149\n",
            "------------------------------------------------------------\n",
            "Example 2: sen12floods_s1_labels_46_2019_02_23_0256_0128_A.npz\n",
            " - Truth label : 1  (pos_ratio = 1.000)\n",
            " - CNN         : pred=1  prob=0.9379\n",
            " - RandomForest: pred=1   prob=0.6600\n",
            " - SVM         : pred=0  prob=0.4698\n",
            "------------------------------------------------------------\n",
            "Example 3: sen12floods_s1_labels_54_2019_03_14_0128_0000_A.npz\n",
            " - Truth label : 1  (pos_ratio = 0.978)\n",
            " - CNN         : pred=0  prob=0.3494\n",
            " - RandomForest: pred=0   prob=0.3184\n",
            " - SVM         : pred=0  prob=0.4145\n",
            "------------------------------------------------------------\n",
            "Example 4: sen12floods_s1_labels_58_2019_04_14_0000_0256_A.npz\n",
            " - Truth label : 0  (pos_ratio = 0.000)\n",
            " - CNN         : pred=0  prob=0.0622\n",
            " - RandomForest: pred=0   prob=0.0733\n",
            " - SVM         : pred=0  prob=0.4733\n",
            "------------------------------------------------------------\n",
            "Example 5: sen12floods_s1_labels_0140_2019_03_13_0000_0000_A.npz\n",
            " - Truth label : 1  (pos_ratio = 0.981)\n",
            " - CNN         : pred=1  prob=0.6317\n",
            " - RandomForest: pred=1   prob=0.5888\n",
            " - SVM         : pred=0  prob=0.4514\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}